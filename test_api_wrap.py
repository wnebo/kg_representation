# my_llm_wrapper.py

from typing import Optional, List
from langchain_core.language_models.llms import LLM
import requests


class RagasJudgeLLM(LLM):
    """
    å°è£…ä¸€ä¸ªå¸¦ header çš„ç®€å• LLM APIï¼Œå…¼å®¹ LangChain å’Œ RAGASã€‚
    """
    url: str  # ä½ çš„ API åœ°å€ï¼Œæ¯”å¦‚ http://localhost:8000/generate

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """
        å‘ API å‘é€è¯·æ±‚ï¼Œæå– content å­—æ®µä½œä¸ºæ¨¡å‹è¾“å‡ºã€‚
        """
        try:
            payload = {"prompt": prompt}
            headers = {
                "Content-Type": "application/json"  # æ˜ç¡®å‘Šè¯‰æœåŠ¡å™¨æˆ‘ä»¬å‘é€çš„æ˜¯ JSON
            }

            response = requests.post(self.url, json=payload, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()

            content = data.get("content", "").strip()
            return content

        except Exception as e:
            raise ValueError(f"è°ƒç”¨ LLM API å‡ºé”™: {e}")
    async def _acall(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """
        å¼‚æ­¥è°ƒç”¨ï¼šç”¨äº RAGAS æˆ– LangChain ä¸­çš„ async æµç¨‹ã€‚
        """
        try:
            payload = {"prompt": prompt}
            headers = {"Content-Type": "application/json"}
            async with aiohttp.ClientSession() as session:
                async with session.post(self.url, json=payload, headers=headers, timeout=30) as response:
                    if response.status != 200:
                        raise ValueError(f"HTTPé”™è¯¯: {response.status}")
                    data = await response.json()
                    content = data.get("content", "").strip()
                    return content
        except Exception as e:
            raise ValueError(f"è°ƒç”¨ LLM API å¼‚æ­¥å‡ºé”™: {e}")


    async def agenerate_prompt(self, prompts: List) -> dict:
        # ğŸ‘‡ æ‰‹åŠ¨æŠŠ PromptValue è½¬æ¢æˆ str
        clean_prompts = [
            p.to_string() if hasattr(p, "to_string") else str(p)
            for p in prompts
        ]
        return await super().agenerate_prompt(clean_prompts)

    @property
    def _llm_type(self) -> str:
        return "ragas_judge_llm"
