{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d78832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import faiss\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "# pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8439a8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "PyTorch Geometric is ready!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "print(\"PyTorch Geometric is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4de6bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸåˆ›å»º entities.csv å’Œ relations.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# åˆ›å»ºå®ä½“æ•°æ®\n",
    "entities = [\n",
    "    {\"entity_name\": \"Apple\", \"type\": \"Company\", \"description\": \"A technology company known for iPhones and MacBooks.\"},\n",
    "    {\"entity_name\": \"Steve Jobs\", \"type\": \"Person\", \"description\": \"The co-founder of Apple.\"},\n",
    "    {\"entity_name\": \"iPhone\", \"type\": \"Product\", \"description\": \"A smartphone product line developed by Apple.\"},\n",
    "    {\"entity_name\": \"Tim Cook\", \"type\": \"Person\", \"description\": \"The CEO of Apple after Steve Jobs.\"},\n",
    "    {\"entity_name\": \"MacBook\", \"type\": \"Product\", \"description\": \"A line of laptop computers developed by Apple.\"},\n",
    "]\n",
    "\n",
    "# åˆ›å»ºå…³ç³»æ•°æ®\n",
    "relations = [\n",
    "    {\"source\": \"Steve Jobs\", \"target\": \"Apple\", \"description\": \"Steve Jobs co-founded Apple in 1976 and helped it become a global brand\"},\n",
    "    {\"source\": \"Apple\", \"target\": \"iPhone\", \"description\": \"Apple released the first iPhone in 2007, revolutionizing the smartphone market\"},\n",
    "]\n",
    "\n",
    "# ä¿å­˜ä¸º CSV æ–‡ä»¶\n",
    "entities_df = pd.DataFrame(entities)\n",
    "relations_df = pd.DataFrame(relations)\n",
    "\n",
    "entities_df.to_csv(\"entities.csv\", index=False)\n",
    "relations_df.to_csv(\"relations.csv\", index=False)\n",
    "\n",
    "print(\"âœ… æˆåŠŸåˆ›å»º entities.csv å’Œ relations.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22d38400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e962923ac55747b48f9fe2ea7c875ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_hf_nomic_bert.py:   0%|          | 0.00/1.96k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python3.11\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\huggingface_cache\\models--nomic-ai--nomic-bert-2048. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-bert-2048:\n",
      "- configuration_hf_nomic_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a54780e96b54c26918929524b5e76b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_hf_nomic_bert.py:   0%|          | 0.00/104k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-bert-2048:\n",
      "- modeling_hf_nomic_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575644f0d3e14a218c3df955bc89cfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/547M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========= Step 1: Load CSV ========= #\n",
    "entities_df = pd.read_csv('entities.csv')    # contains: entity_name, type, description\n",
    "relations_df = pd.read_csv('relations.csv')  # contains: source, target, description\n",
    "\n",
    "# Encode node names to integer IDs\n",
    "node_encoder = LabelEncoder()\n",
    "entities_df['node_id'] = node_encoder.fit_transform(entities_df['entity_name'])\n",
    "node_name_to_id = dict(zip(entities_df['entity_name'], entities_df['node_id']))\n",
    "\n",
    "# ========= Step 2: Build edge_index ========= #\n",
    "edges = []\n",
    "for _, row in relations_df.iterrows():\n",
    "    src = node_name_to_id[row['source']]\n",
    "    tgt = node_name_to_id[row['target']]\n",
    "    edges.append([src, tgt])\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()  # shape: [2, num_edges]\n",
    "\n",
    "# ========= Step 3: Generate node features using Sentence-BERT ========= #\n",
    "model_id = \"nomic-ai/nomic-embed-text-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_id,trust_remote_code=True)\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "node_descs = entities_df['description'].fillna(\"\").tolist()\n",
    "node_features = torch.tensor(model.encode(node_descs), dtype=torch.float)  # shape: [num_nodes, emb_dim]\n",
    "\n",
    "\n",
    "edge_descs = relations_df['description'].fillna(\"\").tolist()\n",
    "# edge_features = model.encode(edge_descs, convert_to_tensor=True)  # shape: [num_edges, emb_dim] \n",
    "# set features on the device by default\n",
    "edge_features = torch.tensor(model.encode(edge_descs, dtype=torch.float))  # shape: [num_edges, emb_dim] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daa85ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08528187,  0.01041333,  0.00754764, ..., -0.09328268,\n",
       "         0.11823732,  0.0499921 ],\n",
       "       [-0.00147327,  0.02058624,  0.00411942, ..., -0.08852488,\n",
       "         0.08957277,  0.03908511],\n",
       "       [-0.11820599,  0.04054209,  0.03426793, ...,  0.00278324,\n",
       "         0.12413181,  0.05148866],\n",
       "       [-0.0102783 ,  0.01969168,  0.04184995, ..., -0.04358399,\n",
       "         0.08945554,  0.05478818],\n",
       "       [-0.03482116,  0.02066975,  0.00669234, ..., -0.00924592,\n",
       "         0.09446841,  0.0352484 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode(node_descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1066253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Step 4: Build PyG Data ========= #\n",
    "data = Data(\n",
    "    x=node_features,  # [num_nodes, node_dim]\n",
    "    edge_index=edge_index,  # [2, num_edges]\n",
    "    edge_attr=edge_features  # [num_edges, emb_dim]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff6ef975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Step 5: Define GraphSAGE ========= #\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model_gnn = GraphSAGE(in_channels=node_features.size(1), hidden_channels=128, out_channels=64)\n",
    "optimizer = torch.optim.Adam(model_gnn.parameters(), lr=0.01)\n",
    "\n",
    "class EdgeAwareGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, edge_dim, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, edge_dim=edge_dim)\n",
    "        self.conv2 = GATConv(hidden_channels, out_channels, edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.conv1(x, edge_index, edge_attr).relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return x\n",
    "    \n",
    "model_gnn = EdgeAwareGNN(in_channels=node_features.size(1), edge_dim=edge_features.size(1), hidden_channels=128, out_channels=node_features.size(1))\n",
    "optimizer = torch.optim.Adam(model_gnn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f7bbb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5632\n",
      "Epoch 2, Loss: 0.5555\n",
      "Epoch 3, Loss: 0.3002\n",
      "Epoch 4, Loss: 0.2061\n",
      "Epoch 5, Loss: 0.1568\n",
      "Epoch 6, Loss: 0.1132\n",
      "Epoch 7, Loss: 0.0897\n",
      "Epoch 8, Loss: 0.0892\n",
      "Epoch 9, Loss: 0.0952\n",
      "Epoch 10, Loss: 0.0961\n",
      "Epoch 11, Loss: 0.0899\n",
      "Epoch 12, Loss: 0.0782\n",
      "Epoch 13, Loss: 0.0653\n",
      "Epoch 14, Loss: 0.0564\n",
      "Epoch 15, Loss: 0.0542\n",
      "Epoch 16, Loss: 0.0552\n",
      "Epoch 17, Loss: 0.0547\n",
      "Epoch 18, Loss: 0.0505\n",
      "Epoch 19, Loss: 0.0443\n",
      "Epoch 20, Loss: 0.0396\n"
     ]
    }
   ],
   "source": [
    "# ========= Step 6: Train GNN (unsupervised, simple) ========= #\n",
    "model_gnn.train()\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    # out = model_gnn(data.x, data.edge_index)\n",
    "    out = model_gnn(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = torch.mean(out.norm(dim=1))  # Dummy regularization loss to \"move\" weights\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ========= Step 7: Extract final embeddings ========= #\n",
    "model_gnn.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings = model_gnn(data.x, data.edge_index, data.edge_attr)\n",
    "    embeddings_np = embeddings.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82be5eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.x device: cpu\n",
      "data.edge_index device: cpu\n",
      "data.edge_attr device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"data.x device:\", data.x.device)\n",
    "print(\"data.edge_index device:\", data.edge_index.device)\n",
    "print(\"data.edge_attr device:\", data.edge_attr.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf52d5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "print(data.edge_index.shape)  # (2, num_edges)\n",
    "# print(data.edge_index)\n",
    "print(data.edge_attr.shape)  # (num_edges, edge_feature_dim)\n",
    "# print(data.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ceecec48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” search: 'who is the founder of apple company ?'\n",
      "dim of search vec: torch.Size([1, 384])\n",
      "dim of FAISS index: 384\n",
      "['Apple', 'Steve Jobs', 'iPhone']\n"
     ]
    }
   ],
   "source": [
    "# ========= Step 8: Build FAISS index for retrieval ========= #\n",
    "faiss_index = faiss.IndexFlatL2(embeddings_np.shape[1])\n",
    "faiss_index.add(embeddings_np)\n",
    "\n",
    "# ========= Step 9: Test search ========= #\n",
    "def search(query, top_k=3):\n",
    "    query_vec = model.encode([query])\n",
    "    query_vec = torch.tensor(query_vec, dtype=torch.float)\n",
    "    print(\"dim of search vec:\", query_vec.shape)  # æ‰“å°æŸ¥è¯¢å‘é‡çš„å½¢çŠ¶\n",
    "    print(\"dim of FAISS index:\", faiss_index.d)  # æ‰“å° FAISS ç´¢å¼•çš„ç»´åº¦\n",
    "\n",
    "    D, I = faiss_index.search(query_vec.numpy(), top_k)\n",
    "    results = [entities_df.iloc[i]['entity_name'] for i in I[0]]\n",
    "    return results\n",
    "\n",
    "# ========= Example ========= #\n",
    "print(\"ğŸ” search: 'who is the founder of apple company ?'\")\n",
    "print(search(\"who is the founder of apple company ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c377f9ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'entities.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# åŠ è½½æ•°æ®\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentities.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m     entities \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelations.json\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'entities.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "with open(\"entities.json\") as f:\n",
    "    entities = json.load(f)\n",
    "with open(\"relations.json\") as f:\n",
    "    relations = json.load(f)\n",
    "\n",
    "# è½¬ä¸º DataFrame\n",
    "entity_df = pd.DataFrame(entities, columns=[\"name\", \"type\", \"desc\"])\n",
    "edge_df = pd.DataFrame(relations, columns=[\"source\", \"target\", \"desc\"])\n",
    "\n",
    "# æ¸…ç†è¾¹ï¼šåªä¿ç•™sourceå’Œtargetéƒ½åœ¨entity_dfä¸­nameåˆ—é‡Œçš„\n",
    "valid_names = set(entity_df[\"name\"])\n",
    "clean_edge_df = edge_df[edge_df[\"source\"].isin(valid_names) & edge_df[\"target\"].isin(valid_names)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0d9cac8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'edge_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# æ‰¾å‡ºedge_dfä¸­æ‰€æœ‰sourceå’Œtarget\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m edge_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43medge_df\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mset\u001b[39m(edge_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m known_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(entity_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# ç¼ºå¤±çš„å®ä½“å\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'edge_df' is not defined"
     ]
    }
   ],
   "source": [
    "# æ‰¾å‡ºedge_dfä¸­æ‰€æœ‰sourceå’Œtarget\n",
    "edge_nodes = set(edge_df[\"source\"]) | set(edge_df[\"target\"])\n",
    "known_nodes = set(entity_df[\"name\"])\n",
    "\n",
    "# ç¼ºå¤±çš„å®ä½“å\n",
    "missing_nodes = edge_nodes - known_nodes\n",
    "\n",
    "# è¡¥å……è¿™äº›å®ä½“ï¼Œç±»å‹å’Œæè¿°å¯ä»¥ç”¨å ä½ç¬¦\n",
    "missing_entity_rows = [{\"name\": name, \"type\": \"Unknown\", \"desc\": \"Auto-added\"} for name in missing_nodes]\n",
    "extended_entity_df = pd.concat([entity_df, pd.DataFrame(missing_entity_rows)], ignore_index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
